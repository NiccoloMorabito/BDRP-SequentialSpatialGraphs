{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import argparse\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from common_utils.stg_utils import Node, Edge, load_pickle\n",
    "from common_utils.process import graph_to_feature_vector, adj_to_normalized_tensor\n",
    "from anomaly_generation.graph_corruption import Corruptor\n",
    "from embedding_training.transformer_classifier import MyNet\n",
    "\n",
    "VIDEOS_FOLDER = \"../videos/\"\n",
    "GRAPH_FOLDER = \"../graphs/\"\n",
    "\n",
    "TRAINING_GRAPH_FOLDER = \"../data/training_graphs/\"\n",
    "VIDEO_PARAMS_FOLDER = \"../data/video_parameters/\"\n",
    "\n",
    "TRAINING_PICKLE = \"../data/train_dataset.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANOMALY_LABEL = 1.\n",
    "NORMAL_LABEL = 0.\n",
    "NUM_NODE_FEATURES = 85 #TODO see if you can dynamically infer it\n",
    "FRAMES_PER_VIDEOCLIP = 500 # about 15 seconds per clip\n",
    "BATCH_SIZE = 1 #TODO batching problems, not working with > 1\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, videos_folder: str, videoparams_folder: str = None, labels_folder: str = None, frames_per_videoclip: int = None):\n",
    "        assert (videoparams_folder is None) ^ (labels_folder is None), \\\n",
    "            \"You need to specify either the folder for videoparams (to corrupt graphs) for the training set \\\n",
    "                or for the labels for the testing set\"\n",
    "\n",
    "        self.frames_per_videoclip = frames_per_videoclip #to cut the video in videoclips\n",
    "        self.tensored_videoclips = list()\n",
    "        self.labels = list()\n",
    "\n",
    "        if videoparams_folder is not None:\n",
    "            self.__load_data_train(videos_folder, videoparams_folder)\n",
    "        else:\n",
    "            self.__load_data_test(videos_folder, labels_folder)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensored_videoclips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returning features, adjacency for each frame in the videoclip and the videoclip label\n",
    "        return self.tensored_videoclips[idx], self.labels[idx]\n",
    "    \n",
    "    \n",
    "    def __load_data_train(self, training_folder: str, videoparams_folder: str) -> None: \n",
    "        for dataset_path in [file for file in os.listdir(training_folder)]:\n",
    "            print(dataset_path)\n",
    "            with open(os.path.join(training_folder, dataset_path), 'rb') as f:\n",
    "                try:\n",
    "                    dataset = pickle.load(f)\n",
    "                except:\n",
    "                    print(f\"File {dataset_path} skipped\")\n",
    "                    continue\n",
    "            \n",
    "            # video params\n",
    "            dataset_name = dataset_path.split(\"_\")[0]\n",
    "            videoparams_path = os.path.join(videoparams_folder, dataset_name + \"_video_params.pickle\")\n",
    "            video2params = load_pickle(videoparams_path)\n",
    "            \n",
    "            # for each video in the dataset\n",
    "            for videoname, graphs in dataset.items():\n",
    "                video_width, video_height = video2params[videoname][\"width\"], video2params[videoname][\"height\"]\n",
    "\n",
    "                # divide the video in videoclips if specified (many videoclips per videoname), otherwise one video per videoname\n",
    "                if self.frames_per_videoclip and len(graphs) > self.frames_per_videoclip:\n",
    "                    graph_seqs = [graphs[x:x+self.frames_per_videoclip] for x in range(0, len(graphs), self.frames_per_videoclip)]\n",
    "                else:\n",
    "                    graph_seqs = [graphs]\n",
    "                \n",
    "                # each videoclip in pytorch\n",
    "                for graph_seq in graph_seqs:                \n",
    "                    self.__process_and_corrupt_videoclip(graph_seq, video_width, video_height)\n",
    "    \n",
    "    def __process_and_corrupt_videoclip(self, graph_seq: List[nx.Graph], width: int, height: int) -> None:\n",
    "        # NORMAL GRAPHS FOR CURRENT VIDEO\n",
    "        features_and_adjs = self.__graph_seq_to_features_and_adjs(graph_seq)\n",
    "        self.tensored_videoclips.append(features_and_adjs)\n",
    "        self.labels.append(torch.tensor(NORMAL_LABEL))\n",
    "        \n",
    "        # CORRUPTED GRAPHS FOR CURRENT VIDEO\n",
    "        corruptor = Corruptor(frame_width=width, frame_height=height)\n",
    "        corrupted_graph_seq = [corruptor.corrupt_graph(graph) for graph in graph_seq]\n",
    "\n",
    "        corr_features_and_adjs = self.__graph_seq_to_features_and_adjs(corrupted_graph_seq)\n",
    "        self.tensored_videoclips.append(corr_features_and_adjs)\n",
    "        self.labels.append(torch.tensor(ANOMALY_LABEL))\n",
    "    \n",
    "    def __graph_seq_to_features_and_adjs(self, graph_seq: List[nx.Graph]) -> List[torch.Tensor]:\n",
    "        #TODO currently, a starting empty token is added to each sequence\n",
    "        features_and_adjs = [self.__get_empty_token()] + [self.__tensorize(graph) for graph in graph_seq]\n",
    "        # padding\n",
    "        while len(features_and_adjs) != self.frames_per_videoclip + 1: # +1 because of the starting token\n",
    "            features_and_adjs.append(self.__get_empty_token())\n",
    "        \n",
    "        return features_and_adjs\n",
    "                \n",
    "    def __tensorize(self, graph: nx.Graph) -> Tuple(torch.Tensor, torch.Tensor):\n",
    "        if len(graph.nodes)==0:\n",
    "            return self.__get_empty_token()\n",
    "        features = graph_to_feature_vector(graph)\n",
    "        adj = adj_to_normalized_tensor(nx.adjacency_matrix(graph))\n",
    "        return features, adj\n",
    "    \n",
    "    def __get_empty_token(self) -> Tuple(torch.Tensor, torch.Tensor):\n",
    "        num_nodes = 20 #TODO this number in normal/abnormal graphs change everytime, see if defining a constant one is fine (should I put 1?)\n",
    "        empty_features = torch.zeros(num_nodes, NUM_NODE_FEATURES)\n",
    "        empty_adj = torch.zeros(num_nodes, num_nodes)\n",
    "        return empty_features, empty_adj\n",
    "\n",
    "\n",
    "    def __load_data_test(self, test_folder: str, labels_folder: str):\n",
    "        for dataset_path in [file for file in os.listdir(test_folder)]:\n",
    "            print(dataset_path)\n",
    "            with open(os.path.join(test_folder, dataset_path), 'rb') as f:\n",
    "                try:\n",
    "                    dataset = pickle.load(f)\n",
    "                except:\n",
    "                    print(f\"File {dataset_path} skipped\")\n",
    "                    continue\n",
    "            \n",
    "            # labels\n",
    "            dataset_name = dataset_path.split(\"_\")[0]\n",
    "            labels_path = os.path.join(labels_folder, dataset_name + \"_labels.pickle\")\n",
    "            videoname_to_labels = load_pickle(labels_path)\n",
    "            \n",
    "            # for each video in the dataset\n",
    "            for videoname, graphs in dataset.items():\n",
    "                frame_labels = videoname_to_labels[videoname]\n",
    "\n",
    "                # divide the video in videoclips if specified (many videoclips per videoname), otherwise one video per videoname\n",
    "                if self.frames_per_videoclip and len(graphs) > self.frames_per_videoclip:\n",
    "                    graph_seqs = [graphs[x:x+self.frames_per_videoclip] for x in range(0, len(graphs), self.frames_per_videoclip)]\n",
    "                    labels = [self.__frame_labels_to_videoclip_label(labels[x:x+self.frames_per_videoclip]) \\\n",
    "                        for x in range(0, len(labels), self.frames_per_videoclip)]\n",
    "                else:\n",
    "                    graph_seqs = [graphs]\n",
    "                    labels = [self.__frame_labels_to_videoclip_label(labels)]\n",
    "                \n",
    "                # each videoclip in pytorch\n",
    "                for graph_seq, label in zip(graph_seqs, labels):                \n",
    "                    self.__process_videoclip(graph_seq, label)\n",
    "    \n",
    "    def __frame_labels_to_videoclip_label(self, frame_labels: List[int]) -> int:\n",
    "        ''' Here the rule on how to label a video as anomaly according to the frame labels'''\n",
    "        #TODO for the moment, if there is at least one anomalous frame, the whole videoclip is considered anomalous\n",
    "        if sum(frame_labels) > 0:\n",
    "            return ANOMALY_LABEL\n",
    "        return NORMAL_LABEL\n",
    "    \n",
    "    def __process_videoclip(self, graph_seq, label) -> None:\n",
    "        features_and_adjs = self.__graph_seq_to_features_and_adjs(graph_seq)\n",
    "        self.tensored_videoclips.append(features_and_adjs)\n",
    "        self.labels.append(torch.tensor(label))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = GraphDataset(videos_folder=TRAINING_GRAPH_FOLDER, videoparams_folder=VIDEO_PARAMS_FOLDER, frames_per_videoclip=FRAMES_PER_VIDEOCLIP)\n",
    "torch.save(train_set, TRAINING_PICKLE)\n",
    "#train_set = GraphDataset(?????)\n",
    "print(f\"Number of clips: {len(train_set)}\")\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "#test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clips: 1202\n"
     ]
    }
   ],
   "source": [
    "train_set = torch.load(TRAINING_PICKLE)\n",
    "print(f\"Number of clips: {len(train_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = torch.utils.data.random_split(train_set, [1000, 202]) #TODO delete (test set will need to have actual anomalies)\n",
    "train_set, test_set = prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor batch in iter(train_dataloader):\\n    seq, label = batch #len(seq) = FRAMES_PER_VIDEOCLIP + 1\\n    #print(label.shape)\\n    features, adj = seq[0]\\n    #print(features.shape)\\n    #print(adj.shape)\\n    #break\\n\\ntorch.Size([2])\\ntorch.Size([2, 20, 85])\\ntorch.Size([2, 20, 20])\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn_multi(batch):\n",
    "    seq_features_and_adjs = [[(features.to(DEVICE), adj.to(DEVICE)) for features, adj in item[0]] for item in batch]\n",
    "    label = [item[1].to(DEVICE) for item in batch]\n",
    "    #seq_features_and_adjs = [item[0] for item in batch]\n",
    "    #label = [item[1] for item in batch]\n",
    "    return [seq_features_and_adjs, label]\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)#, collate_fn=collate_fn) #TODO fix batching problem\n",
    "# this collate function works but the problem is when I need to encode one frame at a time (How to do that?)\n",
    "\n",
    "'''\n",
    "for batch in iter(train_dataloader):\n",
    "    seq, label = batch #len(seq) = FRAMES_PER_VIDEOCLIP + 1\n",
    "    #print(label.shape)\n",
    "    features, adj = seq[0]\n",
    "    #print(features.shape)\n",
    "    #print(adj.shape)\n",
    "    #break\n",
    "\n",
    "torch.Size([2])\n",
    "torch.Size([2, 20, 85])\n",
    "torch.Size([2, 20, 20])\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training\n",
      "epoch=0 - epoch_loss=709.831566631794, epoch accuracy=0.492000013589859\n",
      "epoch=1 - epoch_loss=680.4254964590073, epoch accuracy=0.5379999876022339\n",
      "epoch=2 - epoch_loss=617.6874365955591, epoch accuracy=0.6549999713897705\n",
      "epoch=3 - epoch_loss=546.8410111367702, epoch accuracy=0.6990000009536743\n",
      "epoch=4 - epoch_loss=448.33768977224827, epoch accuracy=0.8550000190734863\n",
      "epoch=5 - epoch_loss=265.2089141011238, epoch accuracy=0.9670000076293945\n",
      "epoch=6 - epoch_loss=72.88722751662135, epoch accuracy=0.9959999918937683\n",
      "epoch=7 - epoch_loss=29.112316742772236, epoch accuracy=0.996999979019165\n",
      "epoch=8 - epoch_loss=11.58269486844074, epoch accuracy=0.9980000257492065\n",
      "epoch=9 - epoch_loss=18.75365456104919, epoch accuracy=0.9950000047683716\n",
      "epoch=10 - epoch_loss=5.890817425548448, epoch accuracy=0.9990000128746033\n",
      "epoch=11 - epoch_loss=26.95245183447696, epoch accuracy=0.996999979019165\n",
      "epoch=12 - epoch_loss=15.172478379243785, epoch accuracy=0.9980000257492065\n",
      "epoch=13 - epoch_loss=31.43006475294419, epoch accuracy=0.9950000047683716\n",
      "epoch=14 - epoch_loss=38.94688181601231, epoch accuracy=0.9940000176429749\n",
      "epoch=15 - epoch_loss=18.26611748361313, epoch accuracy=0.9980000257492065\n",
      "epoch=16 - epoch_loss=6.81244344676179, epoch accuracy=0.9980000257492065\n",
      "epoch=17 - epoch_loss=2.5580151053225677, epoch accuracy=0.9990000128746033\n",
      "epoch=18 - epoch_loss=10.235522862526452, epoch accuracy=0.996999979019165\n",
      "epoch=19 - epoch_loss=11.930894811229493, epoch accuracy=0.9950000047683716\n",
      "epoch=20 - epoch_loss=3.913883910195864, epoch accuracy=0.9980000257492065\n",
      "epoch=21 - epoch_loss=22.220863100900928, epoch accuracy=0.996999979019165\n",
      "epoch=22 - epoch_loss=1.7568345688957265, epoch accuracy=0.9990000128746033\n",
      "epoch=23 - epoch_loss=14.505831033834653, epoch accuracy=0.9980000257492065\n",
      "epoch=24 - epoch_loss=0.05154433932337099, epoch accuracy=1.0\n",
      "epoch=25 - epoch_loss=13.296929018501373, epoch accuracy=0.996999979019165\n",
      "epoch=26 - epoch_loss=2.7143119401636255, epoch accuracy=0.9990000128746033\n",
      "epoch=27 - epoch_loss=2.4641431129672213, epoch accuracy=0.9980000257492065\n",
      "epoch=28 - epoch_loss=12.281308511817016, epoch accuracy=0.9990000128746033\n",
      "epoch=29 - epoch_loss=10.876504213540471, epoch accuracy=0.9980000257492065\n"
     ]
    }
   ],
   "source": [
    "# source (TODO delete) https://n8henrie.com/2021/08/writing-a-transformer-classifier-in-pytorch/\n",
    "\n",
    "\n",
    "epochs = 15\n",
    "model = MyNet(\n",
    "    features_size=NUM_NODE_FEATURES,\n",
    "    embedding_size=8, #TODO decide a value for embedding vector (MUST BE EVEN and dividible by nhead) a low number is probably enough since most of the features are one-hot encoded ones\n",
    "    max_length=FRAMES_PER_VIDEOCLIP + 1 #TODO because of the starting token\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), lr=lr\n",
    ")\n",
    "torch.manual_seed(0)\n",
    "model.train()\n",
    "\n",
    "print(\"starting training\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_count = 0\n",
    "    for batch in iter(train_dataloader):\n",
    "        # to device\n",
    "        seq_features_and_adjs, label = batch #len(seq) = FRAMES_PER_VIDEOCLIP + 1 (if no batches)\n",
    "        seq_features_and_adjs = [(features.to(DEVICE), adj.to(DEVICE)) for features, adj in seq_features_and_adjs]\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        prediction = model.forward(seq_features_and_adjs)\n",
    "\n",
    "        # loss and step\n",
    "        label = label.unsqueeze(1) # or prediction = prediction.squeeze(1)\n",
    "        loss = criterion(prediction, label)\n",
    "        label = label.squeeze(1) #TODO find a way to avoid squeeze/unsqueeze\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) #TODO should I leave it?\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy and loss computation\n",
    "        prediction = torch.round(prediction) #0 or 1 \n",
    "        correct = (prediction == label).sum()\n",
    "\n",
    "        epoch_correct += correct\n",
    "        epoch_count += label.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"{epoch=} - {epoch_loss=}, epoch accuracy={epoch_correct / epoch_count}\")\n",
    "    #print(f\"{test_epoch_loss=}, \"test epoch accuracy: {test_epoch_correct / test_epoch_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing results: accuracy=1.0\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)#, collate_fn=collate_fn) #TODO fix batching problem\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in iter(test_dataloader):\n",
    "        # to device\n",
    "        seq_features_and_adjs, label = batch\n",
    "        seq_features_and_adjs = [(features.to(DEVICE), adj.to(DEVICE)) for features, adj in seq_features_and_adjs]\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        prediction = model.forward(seq_features_and_adjs)\n",
    "\n",
    "        # accuracy computation\n",
    "        prediction = torch.round(prediction) #0 or 1 \n",
    "        correct = (prediction == label).sum()\n",
    "\n",
    "        correct_preds += correct\n",
    "        total_preds += label.size(0)\n",
    "\n",
    "\n",
    "print(f\"Testing results: accuracy={correct_preds / total_preds}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
