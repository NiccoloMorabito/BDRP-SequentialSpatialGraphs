{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import argparse\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "from typing import List, Tuple\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from common_utils.stg_utils import Node, Edge, load_pickle\n",
    "from common_utils.process import graph_to_feature_vector, adj_to_normalized_tensor\n",
    "from anomaly_generation.graph_corruption import Corruptor\n",
    "from embedding_training.transformer_classifier import MyNet\n",
    "\n",
    "VIDEOS_FOLDER = \"../videos/\"\n",
    "GRAPH_FOLDER = \"../graphs/\"\n",
    "\n",
    "TRAINING_GRAPH_FOLDER = \"../data/training_graphs/\"\n",
    "VIDEO_PARAMS_FOLDER = \"../data/video_parameters/\"\n",
    "TESTING_GRAPH_FOLDER = \"../data/testing_graphs/\"\n",
    "LABELS_FOLDER = \"../data/testing_labels/\"\n",
    "\n",
    "TRAINING_PICKLE = \"../data/train_dataset.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x104b87450>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ANOMALY_LABEL = 1.\n",
    "NORMAL_LABEL = 0.\n",
    "NUM_NODE_FEATURES = 85 #TODO see if you can dynamically infer it\n",
    "FRAMES_PER_VIDEOCLIP = 500 # about 15 seconds per clip\n",
    "BATCH_SIZE = 1 #TODO batching problems, not working with > 1\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphDataset(Dataset):\n",
    "    def __init__(self, videos_folder: str, videoparams_folder: str = None, labels_folder: str = None, frames_per_videoclip: int = None):\n",
    "        assert (videoparams_folder is None) ^ (labels_folder is None), \\\n",
    "            \"You need to specify either the folder for videoparams (to corrupt graphs) for the training set \\\n",
    "                or for the labels for the testing set\"\n",
    "\n",
    "        self.frames_per_videoclip = frames_per_videoclip #to cut the video in videoclips\n",
    "        self.tensored_videoclips = list()\n",
    "        self.labels = list()\n",
    "\n",
    "        if videoparams_folder is not None:\n",
    "            self.__load_data_train(videos_folder, videoparams_folder)\n",
    "        else:\n",
    "            self.__load_data_test(videos_folder, labels_folder)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensored_videoclips)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # returning features, adjacency for each frame in the videoclip and the videoclip label\n",
    "        return self.tensored_videoclips[idx], self.labels[idx]\n",
    "    \n",
    "    \n",
    "    def __load_data_train(self, training_folder: str, videoparams_folder: str) -> None: \n",
    "        for dataset_path in [file for file in os.listdir(training_folder)]:\n",
    "            print(dataset_path)\n",
    "            with open(os.path.join(training_folder, dataset_path), 'rb') as f:\n",
    "                try:\n",
    "                    dataset = pickle.load(f)\n",
    "                except:\n",
    "                    print(f\"File {dataset_path} skipped\")\n",
    "                    continue\n",
    "            \n",
    "            # video params\n",
    "            dataset_name = dataset_path.split(\"_\")[0]\n",
    "            videoparams_path = os.path.join(videoparams_folder, dataset_name + \"_video_params.pickle\")\n",
    "            video2params = load_pickle(videoparams_path)\n",
    "            \n",
    "            # for each video in the dataset\n",
    "            for videoname, graphs in dataset.items():\n",
    "                video_width, video_height = video2params[videoname][\"width\"], video2params[videoname][\"height\"]\n",
    "\n",
    "                # divide the video in videoclips if specified (many videoclips per videoname), otherwise one video per videoname\n",
    "                if self.frames_per_videoclip and len(graphs) > self.frames_per_videoclip:\n",
    "                    graph_seqs = [graphs[x:x+self.frames_per_videoclip] for x in range(0, len(graphs), self.frames_per_videoclip)]\n",
    "                else:\n",
    "                    graph_seqs = [graphs]\n",
    "                \n",
    "                # each videoclip in pytorch\n",
    "                for graph_seq in graph_seqs:                \n",
    "                    self.__process_and_corrupt_videoclip(graph_seq, video_width, video_height)\n",
    "    \n",
    "    def __process_and_corrupt_videoclip(self, graph_seq: List[nx.Graph], width: int, height: int) -> None:\n",
    "        # NORMAL GRAPHS FOR CURRENT VIDEO\n",
    "        features_and_adjs = self.__graph_seq_to_features_and_adjs(graph_seq)\n",
    "        self.tensored_videoclips.append(features_and_adjs)\n",
    "        self.labels.append(torch.tensor(NORMAL_LABEL))\n",
    "        \n",
    "        # CORRUPTED GRAPHS FOR CURRENT VIDEO\n",
    "        corruptor = Corruptor(frame_width=width, frame_height=height)\n",
    "        corrupted_graph_seq = [corruptor.corrupt_graph(graph) for graph in graph_seq]\n",
    "\n",
    "        corr_features_and_adjs = self.__graph_seq_to_features_and_adjs(corrupted_graph_seq)\n",
    "        self.tensored_videoclips.append(corr_features_and_adjs)\n",
    "        self.labels.append(torch.tensor(ANOMALY_LABEL))\n",
    "    \n",
    "    def __graph_seq_to_features_and_adjs(self, graph_seq: List[nx.Graph]) -> List[torch.Tensor]:\n",
    "        #TODO currently, a starting empty token is added to each sequence\n",
    "        features_and_adjs = [self.__get_empty_token()] + [self.__tensorize(graph) for graph in graph_seq]\n",
    "        # padding\n",
    "        while len(features_and_adjs) != self.frames_per_videoclip + 1: # +1 because of the starting token\n",
    "            features_and_adjs.append(self.__get_empty_token())\n",
    "        \n",
    "        return features_and_adjs\n",
    "                \n",
    "    def __tensorize(self, graph: nx.Graph): # -> tuple(torch.Tensor):\n",
    "        if len(graph.nodes)==0:\n",
    "            return self.__get_empty_token()\n",
    "        features = graph_to_feature_vector(graph)\n",
    "        adj = adj_to_normalized_tensor(nx.adjacency_matrix(graph))\n",
    "        return features, adj\n",
    "    \n",
    "    def __get_empty_token(self): # -> tuple(torch.Tensor):\n",
    "        num_nodes = 20 #TODO this number in normal/abnormal graphs change everytime, see if defining a constant one is fine (should I put 1?)\n",
    "        empty_features = torch.zeros(num_nodes, NUM_NODE_FEATURES)\n",
    "        empty_adj = torch.zeros(num_nodes, num_nodes)\n",
    "        return empty_features, empty_adj\n",
    "\n",
    "\n",
    "    def __load_data_test(self, test_folder: str, labels_folder: str):\n",
    "        for dataset_path in [file for file in os.listdir(test_folder)]:\n",
    "            with open(os.path.join(test_folder, dataset_path), 'rb') as f:\n",
    "                try:\n",
    "                    dataset = pickle.load(f)\n",
    "                except:\n",
    "                    print(f\"[Skipped] file {dataset_path}\")\n",
    "                    continue\n",
    "            print(dataset_path)\n",
    "            \n",
    "            # labels\n",
    "            dataset_name = dataset_path.split(\"_\")[0]\n",
    "            labels_path = os.path.join(labels_folder, dataset_name + \"_testing_labels.pickle\")\n",
    "            videoname_to_labels = load_pickle(labels_path)\n",
    "            \n",
    "            # for each video in the dataset\n",
    "            for videoname, graphs in dataset.items():\n",
    "                frame_labels = videoname_to_labels[videoname]\n",
    "\n",
    "                # divide the video in videoclips if specified (many videoclips per videoname), otherwise one video per videoname\n",
    "                if self.frames_per_videoclip and len(graphs) > self.frames_per_videoclip:\n",
    "                    graph_seqs = [graphs[x:x+self.frames_per_videoclip] for x in range(0, len(graphs), self.frames_per_videoclip)]\n",
    "                    labels = [self.__frame_labels_to_videoclip_label(frame_labels[x:x+self.frames_per_videoclip]) \\\n",
    "                        for x in range(0, len(frame_labels), self.frames_per_videoclip)]\n",
    "                else:\n",
    "                    graph_seqs = [graphs]\n",
    "                    labels = [self.__frame_labels_to_videoclip_label(labels)]\n",
    "                \n",
    "                # each videoclip in pytorch\n",
    "                for graph_seq, label in zip(graph_seqs, labels):                \n",
    "                    self.__process_videoclip(graph_seq, label)\n",
    "    \n",
    "    def __frame_labels_to_videoclip_label(self, frame_labels: List[int]) -> int:\n",
    "        ''' Here the rule on how to label a video as anomaly according to the frame labels'''\n",
    "        #TODO for the moment, if there is at least one anomalous frame, the whole videoclip is considered anomalous\n",
    "        if sum(frame_labels) > 0:\n",
    "            return ANOMALY_LABEL\n",
    "        return NORMAL_LABEL\n",
    "    \n",
    "    def __process_videoclip(self, graph_seq, label) -> None:\n",
    "        features_and_adjs = self.__graph_seq_to_features_and_adjs(graph_seq)\n",
    "        self.tensored_videoclips.append(features_and_adjs)\n",
    "        self.labels.append(torch.tensor(label))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "File .DS_Store skipped\n",
      "ShanghaiTechDatasetResults_training.pickle\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/video_parameters/ShanghaiTechDatasetResults_video_params.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m''' CREATING THE TRAINING SET '''\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_set \u001b[39m=\u001b[39m GraphDataset(videos_folder\u001b[39m=\u001b[39;49mTRAINING_GRAPH_FOLDER, videoparams_folder\u001b[39m=\u001b[39;49mVIDEO_PARAMS_FOLDER, frames_per_videoclip\u001b[39m=\u001b[39;49mFRAMES_PER_VIDEOCLIP)\n\u001b[1;32m      3\u001b[0m \u001b[39m#torch.save(train_set, TRAINING_PICKLE)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of clips: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(train_set)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m, in \u001b[0;36mGraphDataset.__init__\u001b[0;34m(self, videos_folder, videoparams_folder, labels_folder, frames_per_videoclip)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m()\n\u001b[1;32m     11\u001b[0m \u001b[39mif\u001b[39;00m videoparams_folder \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load_data_train(videos_folder, videoparams_folder)\n\u001b[1;32m     13\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__load_data_test(videos_folder, labels_folder)\n",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mGraphDataset.__load_data_train\u001b[0;34m(self, training_folder, videoparams_folder)\u001b[0m\n\u001b[1;32m     35\u001b[0m dataset_name \u001b[39m=\u001b[39m dataset_path\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     36\u001b[0m videoparams_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(videoparams_folder, dataset_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_video_params.pickle\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m video2params \u001b[39m=\u001b[39m load_pickle(videoparams_path)\n\u001b[1;32m     39\u001b[0m \u001b[39m# for each video in the dataset\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mfor\u001b[39;00m videoname, graphs \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/Library/CloudStorage/GoogleDrive-morabito.1808746@studenti.uniroma1.it/My Drive/BDMA/Semester3 CS/Big Data Research Project/BDRP-SequentialSpatialGraphs/src/common_utils/stg_utils.py:71\u001b[0m, in \u001b[0;36mload_pickle\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_pickle\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mdict\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(path, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     72\u001b[0m         \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/video_parameters/ShanghaiTechDatasetResults_video_params.pickle'"
     ]
    }
   ],
   "source": [
    "''' CREATING THE TRAINING SET '''\n",
    "train_set = GraphDataset(videos_folder=TRAINING_GRAPH_FOLDER, videoparams_folder=VIDEO_PARAMS_FOLDER, frames_per_videoclip=FRAMES_PER_VIDEOCLIP)\n",
    "#torch.save(train_set, TRAINING_PICKLE)\n",
    "print(f\"Number of clips: {len(train_set)}\")\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' LOADING THE TRAINING SET '''\n",
    "train_set = torch.load(TRAINING_PICKLE)\n",
    "print(f\"Number of clips: {len(train_set)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_multi(batch):\n",
    "    seq_features_and_adjs = [[(features.to(DEVICE), adj.to(DEVICE)) for features, adj in item[0]] for item in batch]\n",
    "    label = [item[1].to(DEVICE) for item in batch]\n",
    "    #seq_features_and_adjs = [item[0] for item in batch]\n",
    "    #label = [item[1] for item in batch]\n",
    "    return [seq_features_and_adjs, label]\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)#, collate_fn=collate_fn) #TODO fix batching problem\n",
    "# this collate function works but the problem is when I need to encode one frame at a time (How to do that?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' CREATING THE TEST SET '''\n",
    "test_set = GraphDataset(videos_folder=TESTING_GRAPH_FOLDER, labels_folder=LABELS_FOLDER, frames_per_videoclip=FRAMES_PER_VIDEOCLIP)\n",
    "print(f\"Number of clips: {len(test_set)}, with {len([l for _, l in test_set if l.item()==0.])} normal videos \\\n",
    "    and {len([l for _, l in test_set if l.item()==1.])} real anomalies\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "model = MyNet(\n",
    "    features_size=NUM_NODE_FEATURES,\n",
    "    embedding_size=8, #TODO decide a value for embedding vector (MUST BE EVEN and dividible by nhead) a low number is probably enough since most of the features are one-hot encoded ones\n",
    "    max_length=FRAMES_PER_VIDEOCLIP + 1 #TODO because of the starting token\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(\n",
    "    (p for p in model.parameters() if p.requires_grad), lr=lr\n",
    ")\n",
    "model.train()\n",
    "\n",
    "print(\"starting training\")\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_count = 0\n",
    "    for batch in iter(train_dataloader):\n",
    "        # to device\n",
    "        seq_features_and_adjs, label = batch #len(seq) = FRAMES_PER_VIDEOCLIP + 1 (if no batches)\n",
    "        seq_features_and_adjs = [(features.to(DEVICE), adj.to(DEVICE)) for features, adj in seq_features_and_adjs]\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        prediction = model.forward(seq_features_and_adjs)\n",
    "\n",
    "        # loss and step\n",
    "        label = label.unsqueeze(1) # or prediction = prediction.squeeze(1)\n",
    "        loss = criterion(prediction, label)\n",
    "        label = label.squeeze(1) #TODO find a way to avoid squeeze/unsqueeze\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5) #TODO should I leave it?\n",
    "        optimizer.step()\n",
    "\n",
    "        # accuracy and loss computation\n",
    "        prediction = torch.round(prediction) #0 or 1 \n",
    "        correct = (prediction == label).sum()\n",
    "\n",
    "        epoch_correct += correct\n",
    "        epoch_count += label.size(0)\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"{epoch=} - {epoch_loss=}, epoch accuracy={epoch_correct / epoch_count}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)#, collate_fn=collate_fn) #TODO fix batching problem\n",
    "\n",
    "model.eval()\n",
    "\n",
    "correct_preds = 0\n",
    "total_preds = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in iter(test_dataloader):\n",
    "        # to device\n",
    "        seq_features_and_adjs, label = batch\n",
    "        seq_features_and_adjs = [(features.to(DEVICE), adj.to(DEVICE)) for features, adj in seq_features_and_adjs]\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        prediction = model.forward(seq_features_and_adjs)\n",
    "\n",
    "        # accuracy computation\n",
    "        prediction = torch.round(prediction) #0 or 1 \n",
    "        correct = (prediction == label).sum()\n",
    "\n",
    "        correct_preds += correct\n",
    "        total_preds += label.size(0)\n",
    "\n",
    "\n",
    "print(f\"Testing results: accuracy={correct_preds / total_preds}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
